{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7366a6d1",
   "metadata": {},
   "source": [
    "## RAG란?\n",
    "- 자연어 처리(NLP) 분야의 혁신적인 기술로, 기존 모델의 한계를 넘어서 정보 검색과 생성을 통합하는 방법론\n",
    "- 풍부한 정보를 담고 있는 대규모 문서 데이터베이스에서 관련정보를 검색하고, \n",
    "- 이를 통해 언어 모델이 더 정확하고 상세한 답변을 생성 할 수 있습니다.\n",
    "\n",
    "### 핵심 개념\n",
    "- **검색(Retrieval)**: 질문과 관련된 문서나 정보를 벡터 유사도 기반으로 찾기\n",
    "- **증강(Augmented)**: 찾은 정보를 언어 모델의 컨텍스트에 추가\n",
    "- **생성(Generation)**: 증강된 정보를 바탕으로 더 정확하고 신뢰할 수 있는 답변 생성\n",
    "\n",
    "\n",
    "### RAG의 장점\n",
    "1. 최신 정보 반영 가능 (지식 갱신이 간편)\n",
    "2. 출처 기반 응답 제공\n",
    "3. 도메인 특화 응답 가능\n",
    "\n",
    "### RAG의 기본구조\n",
    "\n",
    "![RAG.png](./images/RAG.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ef186",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**RAG-FLOW**\n",
    "![RAG_Flow.png](./images/RAG_Flow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c800d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## PDF를 문서로 사용해서 RAG 시스템 구축하기\n",
    "- 다음 실습 예시는 기본적인 RAG 구조를 구현합니다.\n",
    "- 추후, 개발을 진행할때는 각각의 구현 상황에 맞는 모듈로 바꾸는 것 만으로도 여러분들만의 RAG 시스템을 구축 할 수 있습니다.\n",
    "\n",
    "- 예시  \n",
    "지금 예시에서는 PDF를 읽어와 문서를 저장하고 있지만, 제공되는 문서가 Text일 경우 Docuemnt Loader 모듈을 바꾸어 사용할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c58dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**실습자료**\n",
    "- 제목 : AI.GOV 해외동향 2025-1호\n",
    "- 링크 : [한국지능사회정보진흥원](https://www.innovation.go.kr/ucms/bbs/B0000051/view.do?nttId=17774&menuNo=300145&pageIndex=)  \n",
    "- 출처 : NIA 한국지증정보원원\n",
    "\n",
    "*링크에서 파일을 다운로드 받은후 최상단 data 폴더에 위치시켜주세요*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6cb4a6",
   "metadata": {},
   "source": [
    "### Step 1 : 문서 로드(Document Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_community : Langchain 핵심 기능 외에 커뮤니티가 기여하고 유지보수하는 외부 서비스 패키지지\n",
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdb722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python으로 PDF문서를 빠르고 쉽게 열고 내용을 추출하거나 파일을 수정 및 이미지 저장을 해주는 라이브러리\n",
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af979fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DoucmentLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/[AI.GOV_해외동향]_2025-1호.pdf\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"1번째 문서 내용 : {docs[0].page_content}\")\n",
    "print(f\"총 문서의 갯수 : {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38daab9",
   "metadata": {},
   "source": [
    "메타데이터 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2fcfb",
   "metadata": {},
   "source": [
    "### Step 2 : 문서 분할하기 (Text Splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6acbe",
   "metadata": {},
   "source": [
    "### Character Text Splitter\n",
    "`CharacterTextSplitter`는 가장 기본적인 텍스트 분할 도구입니다.  \n",
    "특정 문자(separator)를 기준으로 텍스트를 나누어 작은 청크(chunk)로 만들어줍니다.\n",
    "\n",
    "**chunk란?**  \n",
    "문장을 분석/처리 하기 쉽게 텍스트를 작은 단위로 나눈 조각을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63719da",
   "metadata": {},
   "source": [
    "기본 사용법\n",
    "\n",
    "CharacterTextSplitter의 기본 구조는 다음과 같습니다:\n",
    "\n",
    "**주요 매개변수**\n",
    "- `separator`: 텍스트를 나눌 기준 문자 (기본값: `\"\\n\\n\"`)\n",
    "- `chunk_size`: 각 청크의 최대 문자 수\n",
    "- `chunk_overlap`: 인접한 청크 간 겹치는 문자 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e431b7",
   "metadata": {},
   "source": [
    "Chunk Overlap이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunck Overlap 이해하기\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "sample_text = \"이것은 오버랩 개념을 설명하기 위한 예시 문장입니다. 문맥이 끊어지지 않도록 도와줍니다.\"\n",
    "\n",
    "# 1. Overlap 없이 분할하는 Splitter 생성\n",
    "no_overlap_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",         # 띄어쓰기 단위로 분할\n",
    "    chunk_size=10,         # Chunk 최대 크기를 25자로 설정\n",
    "    chunk_overlap=5,       # 겹치는 부분 없음\n",
    ")\n",
    "\n",
    "# 2. 텍스트 분할 실행\n",
    "chunks = no_overlap_splitter.split_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c59855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Overlap이 없을 때 (chunk_overlap=0) ---\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: \\\"{chunk}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Overlap이 있을 때 (chunk_overlap=10) ---\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: \\\"{chunk}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d5c75",
   "metadata": {},
   "source": [
    "띄워쓰기가 없는 아주 긴 단어라면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5075eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_word_text = \"이것은테스트입니다슈퍼울트라하이퍼메가캡숑단어\"\n",
    "\n",
    "# chunk_size를 10으로 아주 작게 설정\n",
    "long_word_splitter = CharacterTextSplitter(\n",
    "    # separator=\" \",         # 띄어쓰기 단위로 분할\n",
    "    chunk_size=10,         # Chunk 최대 크기를 10자로 설정\n",
    "    chunk_overlap=0,       # 겹치는 부분 없음\n",
    ")\n",
    "\n",
    "chunks = long_word_splitter.split_text(long_word_text)\n",
    "\n",
    "print(f\"--- chunk_size=10 설정 결과 ---\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} (길이: {len(chunk)}): \\\"{chunk}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2787a33e",
   "metadata": {},
   "source": [
    "### 작동 우선순위 (Hierarchy)\n",
    "- 코드는 설정된 separators 리스트의 왼쪽부터 순서대로 적용하여 자를 위치를 찾습니다.\n",
    "\n",
    "```Python\n",
    "separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "```\n",
    "1순위: 문단 (\\n\\n)\n",
    "- 가장 먼저 전체 문단을 통째로 담으려 시도합니다.  \n",
    "(설정된 크기를 넘지 않으면 문단 전체가 하나의 청크가 됨)  \n",
    "  \n",
    "2순위: 줄바꿈 (\\n)  \n",
    "- 문단이 너무 길면, 줄바꿈 단위로 쪼갭니다.  \n",
    "  \n",
    "3순위: 문장 (. )  \n",
    "- 그래도 길면, 마침표를 기준으로 문장 단위로 쪼갭니다.\n",
    "  \n",
    "4순위: 단어/공백 ( )  \n",
    "- 문장마저 너무 길면, 띄어쓰기 단위로 쪼갭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc5ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- chunk_size=10 설정 결과 ---\n",
      "Chunk 1 (길이: 96): \"인공지능 기술이 발전함에 따라 거대언어모델(LLM)의 활용도가 높아지고 있습니다. LLM은 방대한 데이터를 학습하여 자연스러운 텍스트를 생성하는 데 탁월한 능력을 보입니다.\"\n",
      "Chunk 2 (길이: 156): \"하지만 모델이 학습하지 않은 최신 정보나 기업 내부의 비공개 데이터에 대해서는 답변하지 못하거나, 사실이 아닌 내용을 사실인 것처럼 말하는 환각(Hallucination) 현상이 발생할 수 있습니다. 이러한 한계를 극복하기 위해 등장한 기술이 바로 검색 증강 생성(RAG)입니다.\"\n",
      "Chunk 3 (길이: 142): \"RAG는 사용자의 질문이 들어오면 먼저 외부 데이터베이스에서 관련 문서를 검색합니다. 그 후 검색된 정확한 정보를 프롬프트에 포함하여 LLM이 답변을 생성하도록 유도합니다. 이 과정을 통해 AI는 더욱 신뢰성 있고 구체적인 답변을 제공할 수 있게 됩니다.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # 바꿔써보기\n",
    "\n",
    "# 2. 문서 분할 \n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"], # 기본값\n",
    "    chunk_size=200, # 최대 크기를 설정\n",
    "    chunk_overlap=20 \n",
    ")\n",
    "\n",
    "test_text = \"\"\"인공지능 기술이 발전함에 따라 거대언어모델(LLM)의 활용도가 높아지고 있습니다. LLM은 방대한 데이터를 학습하여 자연스러운 텍스트를 생성하는 데 탁월한 능력을 보입니다.\n",
    "\n",
    "하지만 모델이 학습하지 않은 최신 정보나 기업 내부의 비공개 데이터에 대해서는 답변하지 못하거나, 사실이 아닌 내용을 사실인 것처럼 말하는 환각(Hallucination) 현상이 발생할 수 있습니다. 이러한 한계를 극복하기 위해 등장한 기술이 바로 검색 증강 생성(RAG)입니다.\n",
    "\n",
    "RAG는 사용자의 질문이 들어오면 먼저 외부 데이터베이스에서 관련 문서를 검색합니다. 그 후 검색된 정확한 정보를 프롬프트에 포함하여 LLM이 답변을 생성하도록 유도합니다. 이 과정을 통해 AI는 더욱 신뢰성 있고 구체적인 답변을 제공할 수 있게 됩니다.\"\"\"\n",
    "\n",
    "chunks = recursive_splitter.split_text(test_text)\n",
    "\n",
    "print(f\"--- chunk_size=10 설정 결과 ---\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} (길이: {len(chunk)}): \\\"{chunk}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e2e35",
   "metadata": {},
   "source": [
    "다시 RAG 구성으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab118584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TextSplitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 기본 설정으로 텍스트 분할기 생성\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,     # 각 청크 최대 200자\n",
    "    chunk_overlap=20   # 20자씩 겹침\n",
    ")\n",
    "\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"분할된 청크의수: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322654f",
   "metadata": {},
   "source": [
    "분할결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de342fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_documents[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a70814",
   "metadata": {},
   "source": [
    "### Step 3 : 임베딩 준비하기 (Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4605b7d",
   "metadata": {},
   "source": [
    "#### Embedding이란?\n",
    "**텍스트 → 수치 벡터(vector)** 로 변환하는 작업  \n",
    "즉, 사람의 언어를 컴퓨터가 이해할 수 있는 형식으로 바꾸는 것.\n",
    "\n",
    "`RAG`에서 관련있는 문서를 검색 해 올 수 있어야 하기 때문에, 문서를 벡터화 시켜, 의미가 비슷한 문서를 찾을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfdd2e",
   "metadata": {},
   "source": [
    "#### OpenAIEmbeddings\n",
    "- OpenAIEmbeddings는 LangChain에서 OpenAI의 강력한 임베딩 모델을 사용하기 위한 도구입니다.  \n",
    "\n",
    "\n",
    "**모델별 비교표**\n",
    "\n",
    "| 모델명 | 비용 (1백만 토큰 당) | 기본 벡터 차원 | 특징 및 장단점 |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`text-embedding-3-small`** | **$0.02** | 1536 | **(추천)** **가성비가 가장 뛰어난 최신 모델.** `ada-002`보다 훨씬 저렴하지만 성능(특히 다국어)은 더 우수함. 대부분의 경우에 추천됨. |\n",
    "| **`text-embedding-3-large`** | $0.13 | 3072 | **최고 성능 모델.** 가장 높은 정확도를 제공하며, 미묘하고 복잡한 의미를 파악하는 데 가장 강력함. 비용과 벡터 저장 공간이 더 필요함. |\n",
    "| `text-embedding-ada-002` | $0.10 | 1536 | **(구세대 모델)** 과거에 가장 널리 쓰였던 모델. 특별한 이유가 없다면 이제는 `3-small` 모델 사용이 모든 면에서 유리함. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 환경변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API 환경변수 값 확인\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "print(f\"OPENAI_API_KEY가 설정되어 있나요?: {openai_api_key[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f90d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. OpenAI 임베딩 모델 초기화 (ChatGPT-4o-mini와 호환)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # OpenAI의 최신 임베딩 모델\n",
    "    # API 키는 환경변수 OPENAI_API_KEY에서 자동으로 읽어옵니다\n",
    ")\n",
    "\n",
    "print(f\"모델명: {embeddings.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e77fe8",
   "metadata": {},
   "source": [
    "임베딩 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = embeddings.embed_documents(chunks)\n",
    "\n",
    "print(f\"--- 문서 임베딩 결과 ---\")\n",
    "print(f\"총 {len(document_vectors)}개의 Chunk가 벡터로 변환되었습니다.\")\n",
    "print(f\"각 벡터의 차원(길이): {len(document_vectors[0])}\")\n",
    "print(f\"첫 번째 Chunk의 벡터(일부): {document_vectors[0]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609e838",
   "metadata": {},
   "source": [
    "### Step 4 : Vector Store 생성 및 문서 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56616ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b035f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. VectorStore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(documents=split_documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669860c",
   "metadata": {},
   "source": [
    "검색해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"프랑스 AI Action Summit 투자 유치액\"\n",
    "\n",
    "# 가장 유사한 문서를 찾음\n",
    "# k : 찾아올 갯수\n",
    "docs = db.similarity_search(query, k=2)\n",
    "\n",
    "print(\"[가장 유사한 문서]\\n\" + docs[0].page_content)\n",
    "print(\"[그다음 유사한 문서]\\n\" + docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0a26f",
   "metadata": {},
   "source": [
    "### Step 5 : Retriever 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1feaad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Retriever\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c61be1",
   "metadata": {},
   "source": [
    "### Step 6 : 프롬프트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"당신은 질문에 답변하는 작업을 수행하는 어시스턴트입니다.\n",
    "다음에 제공된 문맥 정보를 바탕으로 질문에 답하세요.\n",
    "정답을 모를 경우, 모른다고만 말하세요.\n",
    "답변은 반드시 한국어로 작성하세요.\n",
    "\n",
    "#문맥:\n",
    "{context}\n",
    "\n",
    "#질문:\n",
    "{question}\n",
    "\n",
    "#답변:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb39b2",
   "metadata": {},
   "source": [
    "### Step 7 : LLM 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 문맥기반으로 대답을 해야하기 때문에 창의성 0으로 설정\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a14b6d",
   "metadata": {},
   "source": [
    "### Step 8 : 체인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(), # 다음 체인으로 값을 그대로 넘김\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 테스트\n",
    "query = \"주요 해외 AI 에이전트 서비스 동향에 대해 알려줘\"\n",
    "response = chain.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd10eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test0116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
